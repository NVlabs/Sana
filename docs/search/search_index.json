{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"\u26a1\ufe0f Efficient High-Resolution Image &amp; Video Generation ICLR 2025 Oral | ICML 2025 | ICCV 2025 Spotlight"},{"location":"#introduction","title":"Introduction","text":"<p>SANA is an efficiency-oriented codebase for high-resolution image and video generation, providing complete training and inference pipelines.</p>"},{"location":"#models","title":"Models","text":"Model Description Sana Efficient text-to-image generation with Linear DiT, up to 4K resolution Sana-1.5 Training-time and inference-time compute scaling Sana-Sprint Few-step generation via sCM (Consistency Model) distillation Sana-Video Efficient video generation with Block Linear Attention LongSana Minute-length real-time video generation (with LongLive)"},{"location":"#key-techniques","title":"Key Techniques","text":"<ul> <li>Linear Attention: Replace vanilla attention with linear attention for efficiency at high resolutions</li> <li>DC-AE: 32\u00d7 image compression (vs. traditional 8\u00d7) to reduce latent tokens</li> <li>Block Causal Linear Attention: Efficient attention for video generation</li> <li>Causal Mix-FFN: Memory-efficient feedforward for long videos</li> <li>Flow-DPM-Solver: Reduce sampling steps with efficient training and sampling</li> <li>sCM Distillation: One/few-step generation with continuous-time consistency distillation</li> </ul>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>\ud83d\ude80 20\u00d7 smaller, 100\u00d7 faster than Flux-12B</li> <li>\ud83d\uddbc\ufe0f Up to 4K resolution image generation</li> <li>\u26a1 One-step inference with Sana-Sprint</li> <li>\ud83d\udcbb &lt; 8GB VRAM with 4-bit quantization</li> <li>\ud83c\udfac Efficient video generation with Sana-Video</li> <li>\u23f1\ufe0f 27 FPS real-time minute-length video with LongSana</li> <li>\ud83d\udce6 Full training &amp; inference codebase</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>git clone https://github.com/NVlabs/Sana.git\ncd Sana\nbash ./environment_setup.sh sana\n</code></pre> <pre><code>import torch\nfrom diffusers import SanaPipeline\n\npipe = SanaPipeline.from_pretrained(\n\"Efficient-Large-Model/SANA1.5_1.6B_1024px_diffusers\",\n    torch_dtype=torch.bfloat16,\n).to(\"cuda\")\n\nimage = pipe(\"a cyberpunk cat\").images[0]\nimage.save(\"sana.png\")\n</code></pre>"},{"location":"#links","title":"Links","text":""},{"location":"4bit_sana/","title":"4-bit Quant","text":""},{"location":"4bit_sana/#4bit-sanapipeline","title":"4bit SanaPipeline","text":""},{"location":"4bit_sana/#1-environment-setup","title":"1. Environment setup","text":"<p>Follow the official SVDQuant-Nunchaku repository to set up the environment. The guidance can be found here.</p>"},{"location":"4bit_sana/#1-1-quantize-sana-with-svdquant-4bit-optional","title":"1-1. Quantize Sana with SVDQuant-4bit (Optional)","text":"<ol> <li>Convert pth to SVDQuant required safetensor</li> </ol> <pre><code>python tools/convert_scripts/convert_sana_to_svdquant.py \\\n      --orig_ckpt_path Efficient-Large-Model/SANA1.5_1.6B_1024px/checkpoints/SANA1.5_1.6B_1024px.pth \\\n      --model_type SanaMS1.5_1600M_P1_D20 \\\n      --dtype bf16 \\\n      --dump_path output/SANA1.5_1.6B_1024px_svdquant_diffusers \\\n      --save_full_pipeline\n</code></pre> <ol> <li>follow the guidance to compress model    Quantization guidance</li> </ol>"},{"location":"4bit_sana/#2-code-snap-for-inference","title":"2. Code snap for inference","text":"<p>Here we show the code snippet for SanaPipeline. For SanaPAGPipeline, please refer to the SanaPAGPipeline section.</p> <pre><code>import torch\nfrom diffusers import SanaPipeline\n\nfrom nunchaku.models.transformer_sana import NunchakuSanaTransformer2DModel\n\ntransformer = NunchakuSanaTransformer2DModel.from_pretrained(\"mit-han-lab/svdq-int4-sana-1600m\")\npipe = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers\",\n    transformer=transformer,\n    variant=\"bf16\",\n    torch_dtype=torch.bfloat16,\n).to(\"cuda\")\n\npipe.text_encoder.to(torch.bfloat16)\npipe.vae.to(torch.bfloat16)\n\nimage = pipe(\n    prompt=\"A cute \ud83d\udc3c eating \ud83c\udf8b, ink drawing style\",\n    height=1024,\n    width=1024,\n    guidance_scale=4.5,\n    num_inference_steps=20,\n    generator=torch.Generator().manual_seed(42),\n).images[0]\nimage.save(\"sana_1600m.png\")\n</code></pre>"},{"location":"4bit_sana/#3-online-demo","title":"3. Online demo","text":"<p>1). Launch the 4bit Sana.</p> <pre><code>python app/app_sana_4bit.py\n</code></pre> <p>2). Compare with BF16 version</p> <p>Refer to the original Nunchaku-Sana. guidance for SanaPAGPipeline</p> <pre><code>python app/app_sana_4bit_compare_bf16.py\n</code></pre>"},{"location":"8bit_sana/","title":"8-bit Quant","text":""},{"location":"8bit_sana/#sanapipeline","title":"SanaPipeline","text":"<p>SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers from NVIDIA and MIT HAN Lab, by Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, Song Han.</p> <p>The abstract from the paper is:</p> <p>We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096\u00d74096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8\u00d7, we trained an AE that can compress images 32\u00d7, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024\u00d71024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released.</p> <p> <p>Make sure to check out the Schedulers guide to learn how to explore the tradeoff between scheduler speed and quality, and see the reuse components across pipelines section to learn how to efficiently load the same components into multiple pipelines.</p> <p></p> <p>This pipeline was contributed by lawrence-cj and chenjy2003. The original codebase can be found here. The original weights can be found under hf.co/Efficient-Large-Model.</p> <p>Available models:</p> Model Recommended dtype <code>Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers</code> <code>torch.bfloat16</code> <code>Efficient-Large-Model/Sana_1600M_1024px_diffusers</code> <code>torch.float16</code> <code>Efficient-Large-Model/Sana_1600M_1024px_MultiLing_diffusers</code> <code>torch.float16</code> <code>Efficient-Large-Model/Sana_1600M_512px_diffusers</code> <code>torch.float16</code> <code>Efficient-Large-Model/Sana_1600M_512px_MultiLing_diffusers</code> <code>torch.float16</code> <code>Efficient-Large-Model/Sana_600M_1024px_diffusers</code> <code>torch.float16</code> <code>Efficient-Large-Model/Sana_600M_512px_diffusers</code> <code>torch.float16</code> <p>Refer to this collection for more information.</p> <p>Note: The recommended dtype mentioned is for the transformer weights. The text encoder and VAE weights must stay in <code>torch.bfloat16</code> or <code>torch.float32</code> for the model to work correctly. Please refer to the inference example below to see how to load the model with the recommended dtype.</p> <p> <p>Make sure to pass the <code>variant</code> argument for downloaded checkpoints to use lower disk space. Set it to <code>\"fp16\"</code> for models with recommended dtype as <code>torch.float16</code>, and <code>\"bf16\"</code> for models with recommended dtype as <code>torch.bfloat16</code>. By default, <code>torch.float32</code> weights are downloaded, which use twice the amount of disk storage. Additionally, <code>torch.float32</code> weights can be downcasted on-the-fly by specifying the <code>torch_dtype</code> argument. Read about it in the docs.</p> <p></p>"},{"location":"8bit_sana/#quantization","title":"Quantization","text":"<p>Quantization helps reduce the memory requirements of very large models by storing model weights in a lower precision data type. However, quantization may have varying impact on video quality depending on the video model.</p> <p>Refer to the Quantization overview to learn more about supported quantization backends and selecting a quantization backend that supports your use case. The example below demonstrates how to load a quantized [<code>SanaPipeline</code>] for inference with bitsandbytes.</p> <pre><code>import torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, SanaTransformer2DModel, SanaPipeline\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, AutoModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = AutoModel.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = SanaTransformer2DModel.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\npipeline.to(device)\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nimage = pipeline(prompt).images[0]\nimage.save(\"sana.png\")\n</code></pre>"},{"location":"8bit_sana/#sanapipeline_1","title":"SanaPipeline","text":"<p>[[autodoc]] SanaPipeline</p> <ul> <li>all</li> <li>call</li> </ul>"},{"location":"8bit_sana/#sanapagpipeline","title":"SanaPAGPipeline","text":"<p>[[autodoc]] SanaPAGPipeline</p> <ul> <li>all</li> <li>call</li> </ul>"},{"location":"8bit_sana/#sanapipelineoutput","title":"SanaPipelineOutput","text":"<p>[[autodoc]] pipelines.sana.pipeline_output.SanaPipelineOutput</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.10.0 (Recommend to use Anaconda or Miniconda)</li> <li>PyTorch &gt;= 2.5.1+cu12.4</li> </ul>"},{"location":"installation/#quick-install","title":"Quick Install","text":"<pre><code>git clone https://github.com/NVlabs/Sana.git\ncd Sana\n\nbash ./environment_setup.sh sana\n# or you can install each components step by step following environment_setup.sh\n</code></pre>"},{"location":"installation/#hardware-requirements","title":"Hardware Requirements","text":"Model VRAM Required Sana-0.6B 9GB Sana-1.6B 12GB 4-bit Quantized &lt; 8GB <p>Note</p> <p>All the tests are done on A100 GPUs. Different GPU versions may vary.</p>"},{"location":"installation/#diffusers-installation","title":"Diffusers Installation","text":"<p>To use Sana with <code>diffusers</code>, make sure to upgrade to the latest version:</p> <pre><code>pip install git+https://github.com/huggingface/diffusers\n</code></pre>"},{"location":"installation/#quick-start-with-diffusers","title":"Quick Start with Diffusers","text":"<pre><code>import torch\nfrom diffusers import SanaPipeline\n\npipe = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/SANA1.5_1.6B_1024px_diffusers\",\n    torch_dtype=torch.bfloat16,\n)\npipe.to(\"cuda\")\n\npipe.vae.to(torch.bfloat16)\npipe.text_encoder.to(torch.bfloat16)\n\nprompt = 'a cyberpunk cat with a neon sign that says \"Sana\"'\nimage = pipe(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    guidance_scale=4.5,\n    num_inference_steps=20,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n)[0]\n\nimage[0].save(\"sana.png\")\n</code></pre>"},{"location":"installation/#optional-docker","title":"Optional: Docker","text":"<pre><code># Build Docker image\ndocker build -t sana .\n\n# Run inference with Docker\ndocker run --gpus all -it sana python scripts/inference.py\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Model Zoo - Choose your model</li> <li>SANA-Sprint - Fast inference mode with 1-4 steps generations</li> <li>SANA-Video - Video Gen with Linear Attention and Linear Block KV-Cache</li> </ul>"},{"location":"longsana/","title":"LongSANA","text":""},{"location":"longsana/#longsana-sana-video-longlive","title":"\ud83c\udfac LongSANA: SANA-Video + LongLive","text":""},{"location":"longsana/#about-longsana","title":"\ud83d\udcfd\ufe0f About LongSANA","text":"<p>LongSANA is the specialized long-video variant of the SANA-Video framework. It is designed to create minute-long, high-resolution (720p) videos in real time.</p> <p>LongSANA's Core Contributions:</p> <ul> <li> <p>Constant-Memory KV Cache: LongSANA addresses the memory explosion typical of long-context generation by reformulating causal linear attention. Instead of storing a growing history of tokens (which scales linearly or quadratically), it maintains a compact, fixed-size recurrent state (comprising the cumulative sum of states and keys). This reduces the memory complexity to $O(1)$ (constant), allowing the model to generate arbitrarily long videos without increasing GPU memory usage.</p> </li> <li> <p>Block-Wise Autoregressive Training: To effectively learn long-term temporal dependencies, the model employs a novel autoregressive training paradigm with Monotonically Increasing SNR Sampler and Improved Self-Forcing.</p> </li> <li> <p>Performance: LongSANA generates 1-minute length video with 35 seconds, achieving 27 FPS generation speed.</p> </li> </ul>"},{"location":"longsana/#how-to-inference","title":"\ud83c\udfc3 How to Inference","text":""},{"location":"longsana/#1-how-to-use-longsana-pipelines-in-diffusers-coming-soon","title":"1. How to use LongSANA Pipelines in <code>\ud83e\udde8diffusers</code> (Coming soon)","text":"<p>[!IMPORTANT]</p> <pre><code>pip install git+https://github.com/huggingface/diffusers\n</code></pre> <pre><code>import torch\nfrom diffusers import LongSanaVideoPipeline, FlowMatchEulerDiscreteScheduler\nfrom diffusers.utils import export_to_video\n\npipe = LongSanaVideoPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana-Video_2B_480p_LongLive_diffusers\",\n    torch_dtype=torch.bfloat16,\n    base_chunk_frames=10,\n    num_cached_blocks=-1,\n)\npipe.scheduler = FlowMatchEulerDiscreteScheduler()\npipe.vae.to(torch.float32)\npipe.text_encoder.to(torch.bfloat16)\npipe.to(\"cuda\")\n\nprompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\nnegative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n\nvideo = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=480,\n    width=832,\n    frames=161,\n    guidance_scale=1.0,\n    timesteps=[1000, 960, 889, 727, 0],\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"longsana.mp4\", fps=16)\n</code></pre>"},{"location":"longsana/#2-inference-with-txt-file","title":"2. Inference with TXT file","text":"<pre><code># Text to Video\n# num_frames = N_seconds x 16 + 1\naccelerate launch --mixed_precision=bf16 \\\n    inference_video_scripts/inference_sana_video.py \\\n    --config=configs/sana_video_config/Sana_2000M_480px_adamW_fsdp_longsana.yaml \\\n    --model_path=hf://Efficient-Large-Model/SANA-Video_2B_480p_LongLive/checkpoints/SANA_Video_2B_480p_LongLive.pth \\\n    --work_dir=output/inference/longsana_480p \\\n    --txt_file=asset/samples/video_prompts_samples.txt \\\n    --dataset=samples --cfg_scale=1.0 --num_frames 321\n</code></pre>"},{"location":"longsana/#how-to-train","title":"\ud83d\udcbb How to Train","text":""},{"location":"longsana/#data-preparation","title":"Data Preparation","text":"<p>Please follow Self-Forcing to download training prompts:</p> <pre><code>mkdir -p data/longsana\nhf download gdhe17/Self-Forcing vidprom_filtered_extended.txt --local-dir data/longsana\n</code></pre>"},{"location":"longsana/#launch-training","title":"Launch Training","text":"<p>LongSANA is trained in three stages: ODE Initialization, Self-Forcing Training and LongSANA Training. For ODE initialization, we directly provide the ODE initialization checkpoint. If you are interested in training this stage by yourself, you may follow the process described in the CausVid repo to generate trajectories and train the model with:</p> <pre><code># `data_path: generated_ode_data_pair_path` in the config file need to be update. This is the data discussed above.\ntorchrun --nnodes=8 --nproc_per_node=8 --rdzv_id=5235 \\\n  --rdzv_backend=c10d \\\n  --rdzv_endpoint $MASTER_ADDR \\\n  train_video_scripts/train_longsana.py \\\n  --config_path configs/sana_video_config/longsana/480ms/ode.yaml \\\n  --wandb_name debug_480p_ode --logdir output/debug_480p_ode\n</code></pre> <p>Self-Forcing Training and LongSANA Training can be implemented with:</p> <pre><code>torchrun --nnodes=8 --nproc_per_node=8 --rdzv_id=5235 \\\n  --rdzv_backend=c10d \\\n  --rdzv_endpoint $MASTER_ADDR \\\n  train_video_scripts/train_longsana.py \\\n  --config_path configs/sana_video_config/longsana/480ms/self_forcing.yaml \\\n  --wandb_name debug_480p_self_forcing --logdir output/debug_480p_self_forcing\n\n\ntorchrun --nnodes=8 --nproc_per_node=8 --rdzv_id=5235 \\\n  --rdzv_backend=c10d \\\n  --rdzv_endpoint $MASTER_ADDR \\\n  train_video_scripts/train_longsana.py \\\n  --config_path configs/sana_video_config/longsana/480ms/longsana.yaml \\\n  --wandb_name debug_480p_longsana --logdir output/debug_480p_longsana\n</code></pre>"},{"location":"metrics_toolkit/","title":"\ud83d\udcbb How to Inference &amp; Test Metrics (FID, CLIP Score, GenEval, DPG-Bench, etc...)","text":"<p>This ToolKit will automatically inference your model and log the metrics results onto wandb as chart for better illustration. We curerntly support:</p> <ul> <li>[x][FID](https://github.com/mseitzer/pytorch-fid) &amp; CLIP-Score</li> <li>[x][GenEval](https://github.com/djghosh13/geneval)</li> <li>[x][DPG-Bench](https://github.com/TencentQQGYLab/ELLA)</li> <li>[x][ImageReward](https://github.com/THUDM/ImageReward/tree/main)</li> </ul>"},{"location":"metrics_toolkit/#0-install-corresponding-env-for-geneval-and-dpg-bench","title":"0. Install corresponding env for GenEval and DPG-Bench","text":"<p>Make sure you can activate the following envs:</p> <ul> <li><code>conda activate geneval</code>(GenEval)</li> <li><code>conda activate dpg</code>(DGB-Bench)</li> </ul>"},{"location":"metrics_toolkit/#01-prepare-data","title":"0.1 Prepare data.","text":"<p>Metirc FID &amp; CLIP-Score on MJHQ-30K</p> <pre><code>from huggingface_hub import hf_hub_download\n\nhf_hub_download(\n  repo_id=\"playgroundai/MJHQ-30K\",\n  filename=\"mjhq30k_imgs.zip\",\n  local_dir=\"data/test/PG-eval-data/MJHQ-30K/\",\n  repo_type=\"dataset\"\n)\n</code></pre> <p>Unzip mjhq30k_imgs.zip into its per-category folder structure.</p> <pre><code>data/test/PG-eval-data/MJHQ-30K/imgs/\n\u251c\u2500\u2500 animals\n\u251c\u2500\u2500 art\n\u251c\u2500\u2500 fashion\n\u251c\u2500\u2500 food\n\u251c\u2500\u2500 indoor\n\u251c\u2500\u2500 landscape\n\u251c\u2500\u2500 logo\n\u251c\u2500\u2500 people\n\u251c\u2500\u2500 plants\n\u2514\u2500\u2500 vehicles\n</code></pre>"},{"location":"metrics_toolkit/#02-prepare-checkpoints","title":"0.2 Prepare checkpoints","text":"<pre><code>huggingface-cli download  Efficient-Large-Model/Sana_1600M_1024px --repo-type model --local-dir ./output/Sana_1600M_1024px --local-dir-use-symlinks False\n</code></pre>"},{"location":"metrics_toolkit/#1-directly-inference-and-metric-a-pth-file","title":"1. directly [Inference and Metric] a .pth file","text":"<pre><code># We provide four scripts for evaluating metrics:\nfid_clipscore_launch=scripts/bash_run_inference_metric.sh\ngeneval_launch=scripts/bash_run_inference_metric_geneval.sh\ndpg_launch=scripts/bash_run_inference_metric_dpg.sh\nimage_reward_launch=scripts/bash_run_inference_metric_imagereward.sh\n\n# Use following format to metric your models:\n# bash $correspoinding_metric_launch $your_config_file_path $your_relative_pth_file_path\n\n# example\nbash $geneval_launch \\\n    configs/sana_config/1024ms/Sana_1600M_img1024.yaml \\\n    output/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth\n</code></pre>"},{"location":"metrics_toolkit/#2-inference-and-metric-a-list-of-pth-files-using-a-txt-file","title":"2. [Inference and Metric] a list of .pth files using a txt file","text":"<p>You can also write all your pth files of a job in one txt file, eg. model_paths.txt</p> <pre><code># Use following format to metric your models, gathering in a txt file:\n# bash $correspoinding_metric_launch $your_config_file_path $your_txt_file_path_containing_pth_path\n\n# We suggest follow the file tree structure in our project for robust experiment\n# example\nbash scripts/bash_run_inference_metric.sh \\\n    configs/sana_config/1024ms/Sana_1600M_img1024.yaml \\\n    asset/model_paths.txt\n</code></pre>"},{"location":"metrics_toolkit/#3-you-will-get-the-following-data-tree","title":"3. You will get the following data tree.","text":"<pre><code>output\n\u251c\u2500\u2500your_job_name/  (everything will be saved here)\n\u2502  \u251c\u2500\u2500config.yaml\n\u2502  \u251c\u2500\u2500train_log.log\n\n\u2502  \u251c\u2500\u2500checkpoints    (all checkpoints)\n\u2502  \u2502  \u251c\u2500\u2500epoch_1_step_6666.pth\n\u2502  \u2502  \u251c\u2500\u2500epoch_1_step_8888.pth\n\u2502  \u2502  \u251c\u2500\u2500......\n\n\u2502  \u251c\u2500\u2500vis    (all visualization result dirs)\n\u2502  \u2502  \u251c\u2500\u2500visualization_file_name\n\u2502  \u2502  \u2502  \u251c\u2500\u2500xxxxxxx.jpg\n\u2502  \u2502  \u2502  \u251c\u2500\u2500......\n\u2502  \u2502  \u251c\u2500\u2500visualization_file_name2\n\u2502  \u2502  \u2502  \u251c\u2500\u2500xxxxxxx.jpg\n\u2502  \u2502  \u2502  \u251c\u2500\u2500......\n\u2502  \u251c\u2500\u2500......\n\n\u2502  \u251c\u2500\u2500metrics    (all metrics testing related files)\n\u2502  \u2502  \u251c\u2500\u2500model_paths.txt  Optional(\ud83d\udc48)(relative path of testing ckpts)\n\u2502  \u2502  \u2502  \u251c\u2500\u2500output/your_job_name/checkpoings/epoch_1_step_6666.pth\n\u2502  \u2502  \u2502  \u251c\u2500\u2500output/your_job_name/checkpoings/epoch_1_step_8888.pth\n\u2502  \u2502  \u251c\u2500\u2500fid_img_paths.txt  Optional(\ud83d\udc48)(name of testing img_dir in vis)\n\u2502  \u2502  \u2502  \u251c\u2500\u2500visualization_file_name\n\u2502  \u2502  \u2502  \u251c\u2500\u2500visualization_file_name2\n\u2502  \u2502  \u251c\u2500\u2500cached_img_paths.txt  Optional(\ud83d\udc48)\n\u2502  \u2502  \u251c\u2500\u2500......\n</code></pre>"},{"location":"model_zoo/","title":"Model Zoo","text":""},{"location":"model_zoo/#1-we-provide-all-the-links-of-sana-pth-and-diffusers-safetensor-below","title":"\ud83d\udd25 1. We provide all the links of Sana pth and diffusers safetensor below","text":""},{"location":"model_zoo/#sana","title":"SANA","text":"Model Reso pth link diffusers Precision Description Sana-0.6B 512px Sana_600M_512px Efficient-Large-Model/Sana_600M_512px_diffusers fp16/fp32 Multi-Language Sana-0.6B 1024px Sana_600M_1024px Efficient-Large-Model/Sana_600M_1024px_diffusers fp16/fp32 Multi-Language Sana-1.6B 512px Sana_1600M_512px Efficient-Large-Model/Sana_1600M_512px_diffusers fp16/fp32 - Sana-1.6B 512px Sana_1600M_512px_MultiLing Efficient-Large-Model/Sana_1600M_512px_MultiLing_diffusers fp16/fp32 Multi-Language Sana-1.6B 1024px Sana_1600M_1024px Efficient-Large-Model/Sana_1600M_1024px_diffusers fp16/fp32 - Sana-1.6B 1024px Sana_1600M_1024px_MultiLing Efficient-Large-Model/Sana_1600M_1024px_MultiLing_diffusers fp16/fp32 Multi-Language Sana-1.6B 1024px Sana_1600M_1024px_BF16 Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers bf16/fp32 Multi-Language Sana-1.6B-int4 1024px - mit-han-lab/svdq-int4-sana-1600m int4 Multi-Language Sana-1.6B 2Kpx Sana_1600M_2Kpx_BF16 Efficient-Large-Model/Sana_1600M_2Kpx_BF16_diffusers bf16/fp32 Multi-Language Sana-1.6B 4Kpx Sana_1600M_4Kpx_BF16 Efficient-Large-Model/Sana_1600M_4Kpx_BF16_diffusers bf16/fp32 Multi-Language ControlNet Sana-1.6B-ControlNet 1Kpx Sana_1600M_1024px_BF16_ControlNet_HED Coming soon bf16/fp32 Multi-Language Sana-0.6B-ControlNet 1Kpx Sana_600M_1024px_ControlNet_HED - soon fp16/fp32 -"},{"location":"model_zoo/#sana-15","title":"SANA-1.5","text":"Model Reso pth link diffusers Precision Description SANA1.5-4.8B 1024px SANA1.5_4.8B_1024px Efficient-Large-Model/SANA1.5_4.8B_1024px_diffusers bf16 Multi-Language SANA1.5-1.6B 1024px SANA1.5_1.6B_1024px Efficient-Large-Model/SANA1.5_1.6B_1024px_diffusers bf16 Multi-Language"},{"location":"model_zoo/#sana-sprint","title":"SANA-Sprint","text":"Model Reso pth link diffusers Precision Description Sana-Sprint-0.6B 1024px Sana-Sprint_0.6B_1024px Efficient-Large-Model/Sana_Sprint_0.6B_1024px_diffusers bf16 Multi-Language Sana-Sprint-1.6B 1024px Sana-Sprint_1.6B_1024px Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers bf16 Multi-Language"},{"location":"model_zoo/#sana-video","title":"SANA-Video","text":"Model Reso pth link diffusers Precision Description Sana-Video-2B 480p Sana-Video_2B_480p Efficient-Large-Model/Sana-Video_2B_480p_diffusers bf16 5s Pre-train model LongSANA-Video-2B 480p SANA-Video_2B_480p_LongLive Efficient-Large-Model/SANA-Video_2B_480p_LongLive_diffusers bf16 27FPS Minute-length model LongSANA-Video-2B-ODE-Init 480p LongSANA_2B_480p_ode --- bf16 LongSANA first step model initialized from ODE trajectories LongSANA-Video-2B-Self-Forcing 480p LongSANA_2B_480p_self_forcing --- bf16 LongSANA second step model trained by Self-Forcing"},{"location":"model_zoo/#2-make-sure-to-use-correct-precisionfp16bf16fp32-for-training-and-inference","title":"\u2757 2. Make sure to use correct precision(fp16/bf16/fp32) for training and inference.","text":""},{"location":"model_zoo/#we-provide-two-samples-to-use-fp16-and-bf16-weights-respectively","title":"We provide two samples to use fp16 and bf16 weights, respectively.","text":"<p>\u2757\ufe0fMake sure to set <code>variant</code> and <code>torch_dtype</code> in diffusers pipelines to the desired precision.</p>"},{"location":"model_zoo/#1-for-fp16-models","title":"1). For fp16 models","text":"<pre><code># run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers\nimport torch\nfrom diffusers import SanaPipeline\n\npipe = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n    variant=\"fp16\",\n    torch_dtype=torch.float16,\n)\npipe.to(\"cuda\")\n\npipe.vae.to(torch.bfloat16)\npipe.text_encoder.to(torch.bfloat16)\n\nprompt = 'a cyberpunk cat with a neon sign that says \"Sana\"'\nimage = pipe(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    guidance_scale=5.0,\n    num_inference_steps=20,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n)[0]\n\nimage[0].save(\"sana.png\")\n</code></pre>"},{"location":"model_zoo/#2-for-bf16-models","title":"2). For bf16 models","text":"<pre><code># run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers\nimport torch\nfrom diffusers import SanaPAGPipeline\n\npipe = SanaPAGPipeline.from_pretrained(\n  \"Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers\",\n  variant=\"bf16\",\n  torch_dtype=torch.bfloat16,\n  pag_applied_layers=\"transformer_blocks.8\",\n)\npipe.to(\"cuda\")\n\npipe.text_encoder.to(torch.bfloat16)\npipe.vae.to(torch.bfloat16)\n\nprompt = 'a cyberpunk cat with a neon sign that says \"Sana\"'\nimage = pipe(\n    prompt=prompt,\n    guidance_scale=5.0,\n    pag_scale=2.0,\n    num_inference_steps=20,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n)[0]\nimage[0].save('sana.png')\n</code></pre>"},{"location":"model_zoo/#3-2k-4k-models","title":"\u2757 3. 2K &amp; 4K models","text":"<p>4K models need VAE tiling to avoid OOM issue.(16 GPU is recommended)</p> <pre><code># run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers\nimport torch\nfrom diffusers import SanaPipeline\n\n# 2K model: Efficient-Large-Model/Sana_1600M_2Kpx_BF16_diffusers\n# 4K model:Efficient-Large-Model/Sana_1600M_4Kpx_BF16_diffusers\npipe = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_4Kpx_BF16_diffusers\",\n    variant=\"bf16\",\n    torch_dtype=torch.bfloat16,\n)\npipe.to(\"cuda\")\n\npipe.vae.to(torch.bfloat16)\npipe.text_encoder.to(torch.bfloat16)\n\n# for 4096x4096 image generation OOM issue, feel free adjust the tile size\nif pipe.transformer.config.sample_size == 128:\n    pipe.vae.enable_tiling(\n        tile_sample_min_height=1024,\n        tile_sample_min_width=1024,\n        tile_sample_stride_height=896,\n        tile_sample_stride_width=896,\n    )\nprompt = 'a cyberpunk cat with a neon sign that says \"Sana\"'\nimage = pipe(\n    prompt=prompt,\n    height=4096,\n    width=4096,\n    guidance_scale=5.0,\n    num_inference_steps=20,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n)[0]\n\nimage[0].save(\"sana_4K.png\")\n</code></pre>"},{"location":"model_zoo/#4-int4-inference","title":"\u2757 4. int4 inference","text":"<p>This int4 model is quantized with SVDQuant-Nunchaku. You need first follow the guidance of installation of nunchaku engine, then you can use the following code snippet to perform inference with int4 Sana model.</p> <p>Here we show the code snippet for SanaPipeline. For SanaPAGPipeline, please refer to the SanaPAGPipeline section.</p> <pre><code>import torch\nfrom diffusers import SanaPipeline\n\nfrom nunchaku.models.transformer_sana import NunchakuSanaTransformer2DModel\n\ntransformer = NunchakuSanaTransformer2DModel.from_pretrained(\"mit-han-lab/svdq-int4-sana-1600m\")\npipe = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers\",\n    transformer=transformer,\n    variant=\"bf16\",\n    torch_dtype=torch.bfloat16,\n).to(\"cuda\")\n\npipe.text_encoder.to(torch.bfloat16)\npipe.vae.to(torch.bfloat16)\n\nimage = pipe(\n    prompt=\"A cute \ud83d\udc3c eating \ud83c\udf8b, ink drawing style\",\n    height=1024,\n    width=1024,\n    guidance_scale=4.5,\n    num_inference_steps=20,\n    generator=torch.Generator().manual_seed(42),\n).images[0]\nimage.save(\"sana_1600m.png\")\n</code></pre>"},{"location":"model_zoo/#5-convert-pth-to-diffusers-safetensor","title":"\ud83d\udd27 5. Convert <code>.pth</code> to diffusers <code>.safetensor</code>","text":"<pre><code>python tools/convert_scripts/convert_sana_to_diffusers.py \\\n      --orig_ckpt_path Efficient-Large-Model/Sana_1600M_1024px_BF16/checkpoints/Sana_1600M_1024px_BF16.pth \\\n      --model_type SanaMS_1600M_P1_D20 \\\n      --dtype bf16 \\\n      --dump_path output/Sana_1600M_1024px_BF16_diffusers \\\n      --save_full_pipeline\n</code></pre>"},{"location":"sana/","title":"SANA","text":""},{"location":"sana/#sana-efficient-high-resolution-image-synthesis-with-linear-diffusion-transformer","title":"\u26a1\ufe0f Sana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer","text":"<p>This guide covers training and inference for Sana text-to-image models.</p>"},{"location":"sana/#hardware-requirements","title":"Hardware Requirements","text":"Task VRAM Inference (0.6B) 9GB Inference (1.6B) 12GB Inference (4-bit) &lt; 8GB Training 32GB <p>Note</p> <p>All tests are done on A100 GPUs. Different GPU versions may vary.</p>"},{"location":"sana/#inference","title":"Inference","text":""},{"location":"sana/#using-diffusers-recommended","title":"Using Diffusers (Recommended)","text":"<pre><code>pip install git+https://github.com/huggingface/diffusers\n</code></pre> <pre><code>import torch\nfrom diffusers import SanaPipeline\n\npipe = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/SANA1.5_1.6B_1024px_diffusers\",\n    torch_dtype=torch.bfloat16,\n)\npipe.to(\"cuda\")\n\npipe.vae.to(torch.bfloat16)\npipe.text_encoder.to(torch.bfloat16)\n\nprompt = 'a cyberpunk cat with a neon sign that says \"Sana\"'\nimage = pipe(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    guidance_scale=4.5,\n    num_inference_steps=20,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n)[0]\n\nimage[0].save(\"sana.png\")\n</code></pre>"},{"location":"sana/#using-sanapagpipeline","title":"Using SanaPAGPipeline","text":"<pre><code>import torch\nfrom diffusers import SanaPAGPipeline\n\npipe = SanaPAGPipeline.from_pretrained(\n    \"Efficient-Large-Model/SANA1.5_1.6B_1024px_diffusers\",\n    torch_dtype=torch.bfloat16,\n    pag_applied_layers=\"transformer_blocks.8\",\n)\npipe.to(\"cuda\")\n\npipe.text_encoder.to(torch.bfloat16)\npipe.vae.to(torch.bfloat16)\n\nimage = pipe(\n    prompt='a cyberpunk cat with a neon sign that says \"Sana\"',\n    guidance_scale=5.0,\n    pag_scale=2.0,\n    num_inference_steps=20,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n)[0]\nimage[0].save('sana.png')\n</code></pre>"},{"location":"sana/#using-native-pipeline","title":"Using Native Pipeline","text":"<pre><code>import torch\nfrom app.sana_pipeline import SanaPipeline\nfrom torchvision.utils import save_image\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngenerator = torch.Generator(device=device).manual_seed(42)\n\nsana = SanaPipeline(\"configs/sana1-5_config/1024ms/Sana_1600M_1024px_allqknorm_bf16_lr2e5.yaml\")\nsana.from_pretrained(\"hf://Efficient-Large-Model/SANA1.5_1.6B_1024px/checkpoints/SANA1.5_1.6B_1024px.pth\")\n\nimage = sana(\n    prompt='a cyberpunk cat with a neon sign that says \"Sana\"',\n    height=1024,\n    width=1024,\n    guidance_scale=4.5,\n    pag_guidance_scale=1.0,\n    num_inference_steps=20,\n    generator=generator,\n)\nsave_image(image, 'output/sana.png', nrow=1, normalize=True, value_range=(-1, 1))\n</code></pre>"},{"location":"sana/#gradio-demo","title":"Gradio Demo","text":"<pre><code>DEMO_PORT=15432 \\\npython app/app_sana.py \\\n    --share \\\n    --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \\\n    --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px_BF16/checkpoints/Sana_1600M_1024px_BF16.pth \\\n    --image_size=1024\n</code></pre>"},{"location":"sana/#batch-inference","title":"Batch Inference","text":"<pre><code># Run samples in a txt file\npython scripts/inference.py \\\n    --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \\\n    --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \\\n    --txt_file=asset/samples/samples_mini.txt\n\n# Run samples in a json file\npython scripts/inference.py \\\n    --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \\\n    --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \\\n    --json_file=asset/samples/samples_mini.json\n</code></pre>"},{"location":"sana/#training","title":"Training","text":""},{"location":"sana/#data-preparation","title":"Data Preparation","text":"<p>Prepare image-text pairs in the following format:</p> <pre><code>asset/example_data\n\u251c\u2500\u2500 AAA.txt\n\u251c\u2500\u2500 AAA.png\n\u251c\u2500\u2500 BCC.txt\n\u251c\u2500\u2500 BCC.png\n\u251c\u2500\u2500 CCC.txt\n\u2514\u2500\u2500 CCC.png\n</code></pre>"},{"location":"sana/#train-from-scratch","title":"Train from Scratch","text":"<pre><code># Train Sana 0.6B with 512x512 resolution\nbash train_scripts/train.sh \\\n    configs/sana_config/512ms/Sana_600M_img512.yaml \\\n    --data.data_dir=\"[asset/example_data]\" \\\n    --data.type=SanaImgDataset \\\n    --model.multi_scale=false \\\n    --train.train_batch_size=32\n</code></pre>"},{"location":"sana/#fine-tuning","title":"Fine-tuning","text":"<pre><code># Fine-tune Sana 1.6B with 1024x1024 resolution\nbash train_scripts/train.sh \\\n    configs/sana_config/1024ms/Sana_1600M_img1024.yaml \\\n    --data.data_dir=\"[asset/example_data]\" \\\n    --data.type=SanaImgDataset \\\n    --model.load_from=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \\\n    --model.multi_scale=false \\\n    --train.train_batch_size=8\n</code></pre>"},{"location":"sana/#multi-scale-webdataset","title":"Multi-Scale WebDataset","text":"<p>Convert data to WebDataset format:</p> <pre><code>python tools/convert_scripts/convert_ImgDataset_to_WebDatasetMS_format.py\n</code></pre> <p>Then train:</p> <pre><code>bash train_scripts/train.sh \\\n    configs/sana_config/512ms/Sana_600M_img512.yaml \\\n    --data.data_dir=\"[asset/example_data_tar]\" \\\n    --data.type=SanaWebDatasetMS \\\n    --model.multi_scale=true \\\n    --train.train_batch_size=32\n</code></pre>"},{"location":"sana/#training-with-fsdp","title":"Training with FSDP","text":"<pre><code># Download toy dataset\nhuggingface-cli download Efficient-Large-Model/toy_data --repo-type dataset --local-dir ./data/toy_data\n\n# DDP training\nbash train_scripts/train.sh \\\n    configs/sana1-5_config/1024ms/Sana_1600M_1024px_allqknorm_bf16_lr2e5.yaml \\\n    --data.data_dir=\"[data/toy_data]\" \\\n    --data.type=SanaWebDatasetMS \\\n    --model.multi_scale=true \\\n    --data.load_vae_feat=true \\\n    --train.train_batch_size=2\n\n# FSDP training\nbash train_scripts/train.sh \\\n    configs/sana1-5_config/1024ms/Sana_1600M_1024px_AdamW_fsdp.yaml \\\n    --data.data_dir=\"[data/toy_data]\" \\\n    --data.type=SanaWebDatasetMS \\\n    --model.multi_scale=true \\\n    --data.load_vae_feat=true \\\n    --train.use_fsdp=true \\\n    --train.train_batch_size=2\n</code></pre>"},{"location":"sana/#related","title":"Related","text":"<ul> <li>Model Zoo - All available models</li> <li>4-bit Sana - Memory-efficient inference</li> <li>LoRA &amp; DreamBooth - Fine-tuning methods</li> </ul>"},{"location":"sana/#citation","title":"Citation","text":"<pre><code>@misc{xie2024sana,\n      title={Sana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer},\n      author={Enze Xie and Junsong Chen and Junyu Chen and Han Cai and Haotian Tang and Yujun Lin and Zhekai Zhang and Muyang Li and Ligeng Zhu and Yao Lu and Song Han},\n      year={2024},\n      eprint={2410.10629},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2410.10629},\n}\n\n@misc{xie2025sana,\n      title={SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer},\n      author={Xie, Enze and Chen, Junsong and Zhao, Yuyang and Yu, Jincheng and Zhu, Ligeng and Lin, Yujun and Zhang, Zhekai and Li, Muyang and Chen, Junyu and Cai, Han and others},\n      year={2025},\n      eprint={2501.18427},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2501.18427},\n}\n</code></pre>"},{"location":"sana_controlnet/","title":"ControlNet","text":""},{"location":"sana_controlnet/#controlnet","title":"\ud83d\udd25 ControlNet","text":"<p>We incorporate a ControlNet-like(https://github.com/lllyasviel/ControlNet) module enables fine-grained control over text-to-image diffusion models. We implement a ControlNet-Transformer architecture, specifically tailored for Transformers, achieving explicit controllability alongside high-quality image generation.</p> <p> </p>"},{"location":"sana_controlnet/#inference-of-sana-controlnet","title":"Inference of <code>Sana + ControlNet</code>","text":""},{"location":"sana_controlnet/#1-gradio-interface","title":"1). Gradio Interface","text":"<pre><code>python app/app_sana_controlnet_hed.py \\\n        --config configs/sana_controlnet_config/Sana_1600M_1024px_controlnet_bf16.yaml \\\n        --model_path hf://Efficient-Large-Model/Sana_1600M_1024px_BF16_ControlNet_HED/checkpoints/Sana_1600M_1024px_BF16_ControlNet_HED.pth\n</code></pre>"},{"location":"sana_controlnet/#2-inference-with-json-file","title":"2). Inference with JSON file","text":"<pre><code>python tools/controlnet/inference_controlnet.py \\\n        --config configs/sana_controlnet_config/Sana_1600M_1024px_controlnet_bf16.yaml \\\n        --model_path hf://Efficient-Large-Model/Sana_1600M_1024px_BF16_ControlNet_HED/checkpoints/Sana_1600M_1024px_BF16_ControlNet_HED.pth \\\n        --json_file asset/controlnet/samples_controlnet.json\n</code></pre>"},{"location":"sana_controlnet/#3-inference-code-snap","title":"3). Inference code snap","text":"<pre><code>import torch\nfrom PIL import Image\nfrom app.sana_controlnet_pipeline import SanaControlNetPipeline\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npipe = SanaControlNetPipeline(\"configs/sana_controlnet_config/Sana_1600M_1024px_controlnet_bf16.yaml\")\npipe.from_pretrained(\"hf://Efficient-Large-Model/Sana_1600M_1024px_BF16_ControlNet_HED/checkpoints/Sana_1600M_1024px_BF16_ControlNet_HED.pth\")\n\nref_image = Image.open(\"asset/controlnet/ref_images/A transparent sculpture of a duck made out of glass. The sculpture is in front of a painting of a la.jpg\")\nprompt = \"A transparent sculpture of a duck made out of glass. The sculpture is in front of a painting of a landscape.\"\n\nimages = pipe(\n    prompt=prompt,\n    ref_image=ref_image,\n    guidance_scale=4.5,\n    num_inference_steps=10,\n    sketch_thickness=2,\n    generator=torch.Generator(device=device).manual_seed(0),\n)\n</code></pre>"},{"location":"sana_controlnet/#training-of-sana-controlnet","title":"Training of <code>Sana + ControlNet</code>","text":""},{"location":"sana_controlnet/#coming-soon","title":"Coming soon","text":""},{"location":"sana_lora_dreambooth/","title":"DreamBooth training example for SANA","text":"<p>DreamBooth is a method to personalize text2image models like stable diffusion given just a few (3~5) images of a subject.</p> <p>The <code>train_dreambooth_lora_sana.py</code> script shows how to implement the training procedure with LoRA and adapt it for SANA.</p> <p>This will also allow us to push the trained model parameters to the Hugging Face Hub platform.</p>"},{"location":"sana_lora_dreambooth/#running-locally-with-pytorch","title":"Running locally with PyTorch","text":""},{"location":"sana_lora_dreambooth/#installing-the-dependencies","title":"Installing the dependencies","text":"<p>Before running the scripts, make sure to install the library's training dependencies:</p> <p>Important</p> <p>To make sure you can successfully run the latest versions of the example scripts, we highly recommend installing from source and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:</p> <pre><code>git clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n</code></pre> <p>And initialize an \ud83e\udd17Accelerate environment with:</p> <pre><code>accelerate config\n</code></pre> <p>Or for a default accelerate configuration without answering questions about your environment</p> <pre><code>accelerate config default\n</code></pre> <p>Or if your environment doesn't support an interactive shell (e.g., a notebook)</p> <pre><code>from accelerate.utils import write_basic_config\nwrite_basic_config()\n</code></pre> <p>When running <code>accelerate config</code>, if we specify torch compile mode to True there can be dramatic speedups. Note also that we use PEFT library as backend for LoRA training, make sure to have <code>peft&gt;=0.14.0</code> installed in your environment.</p>"},{"location":"sana_lora_dreambooth/#dog-toy-example","title":"Dog toy example","text":"<p>Now let's get our dataset. For this example we will use some dog images: https://huggingface.co/datasets/diffusers/dog-example.</p> <p>Let's first download it locally:</p> <pre><code>from huggingface_hub import snapshot_download\n\nlocal_dir = \"data/dreambooth/dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n</code></pre> <p>This will also allow us to push the trained LoRA parameters to the Hugging Face Hub platform.</p> <p>Here is the Model Card for you to choose the desired pre-trained models and set it to <code>MODEL_NAME</code>.</p> <p>Now, we can launch training using file here:</p> <pre><code>bash train_scripts/train_lora.sh\n</code></pre> <p>or you can run it locally:</p> <pre><code>export MODEL_NAME=\"Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers\"\nexport INSTANCE_DIR=\"data/dreambooth/dog\"\nexport OUTPUT_DIR=\"trained-sana-lora\"\n\naccelerate launch --num_processes 8 --main_process_port 29500 --gpu_ids 0,1,2,3 \\\n  train_scripts/train_dreambooth_lora_sana.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"bf16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --use_8bit_adam \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a pond, yarn art style\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n</code></pre> <p>For using <code>push_to_hub</code>, make you're logged into your Hugging Face account:</p> <pre><code>huggingface-cli login\n</code></pre> <p>To better track our training experiments, we're using the following flags in the command above:</p> <ul> <li><code>report_to=\"wandb</code> will ensure the training runs are tracked on Weights and Biases. To use it, be sure to install <code>wandb</code> with <code>pip install wandb</code>. Don't forget to call <code>wandb login &lt;your_api_key&gt;</code> before training if you haven't done it before.</li> <li><code>validation_prompt</code> and <code>validation_epochs</code> to allow the script to do a few validation inference runs. This allows us to qualitatively check if the training is progressing as expected.</li> </ul>"},{"location":"sana_lora_dreambooth/#notes","title":"Notes","text":"<p>Additionally, we welcome you to explore the following CLI arguments:</p> <ul> <li><code>--lora_layers</code>: The transformer modules to apply LoRA training on. Please specify the layers in a comma seperated. E.g. - \"to_k,to_q,to_v\" will result in lora training of attention layers only.</li> <li><code>--complex_human_instruction</code>: Instructions for complex human attention as shown in here.</li> <li><code>--max_sequence_length</code>: Maximum sequence length to use for text embeddings.</li> </ul> <p>We provide several options for optimizing memory optimization:</p> <ul> <li><code>--offload</code>: When enabled, we will offload the text encoder and VAE to CPU, when they are not used.</li> <li><code>cache_latents</code>: When enabled, we will pre-compute the latents from the input images with the VAE and remove the VAE from memory once done.</li> <li><code>--use_8bit_adam</code>: When enabled, we will use the 8bit version of AdamW provided by the <code>bitsandbytes</code> library.</li> </ul> <p>Refer to the official documentation of the <code>SanaPipeline</code> to know more about the models available under the SANA family and their preferred dtypes during inference.</p>"},{"location":"sana_lora_dreambooth/#samples","title":"Samples","text":"<p>We show some samples during Sana-LoRA fine-tuning process below.</p> <p>  training samples at step=0  </p> <p>  training samples at step=500  </p>"},{"location":"sana_sprint/","title":"SANA-Sprint","text":""},{"location":"sana_sprint/#sana-sprint-one-step-diffusion-with-continuous-time-consistency-distillation","title":"\ud83c\udfc3SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation","text":""},{"location":"sana_sprint/#how-to-inference","title":"How to Inference","text":""},{"location":"sana_sprint/#1-how-to-use-sanasprintpipeline-with-diffusers","title":"1. How to use <code>SanaSprintPipeline</code> with <code>\ud83e\udde8diffusers</code>","text":"<p>Important</p> <p>Upgrade your diffusers to use <code>SanaSprintPipeline</code>:</p> <pre><code>```bash\npip install git+https://github.com/huggingface/diffusers\n```\n</code></pre> <pre><code># test sana sprint\nfrom diffusers import SanaSprintPipeline\nimport torch\n\npipeline = SanaSprintPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers\",\n    torch_dtype=torch.bfloat16\n)\n# Use DC-AE-Lite for faster speed.\n# from diffusers import AutoencoderDC\n# vae = AutoencoderDC.from_pretrained(\"mit-han-lab/dc-ae-lite-f32c32-sana-1.1-diffusers\")\n# pipeline.vae = vae\npipeline.to(\"cuda:0\")\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\n\nimage = pipeline(prompt=prompt, num_inference_steps=2).images[0]\nimage.save(\"test_out.png\")\n</code></pre> <pre><code># if you want to compile the vae. You need to upgrade to torch&gt;=2.6.0\n# DCAE1.1: 1287MB/0.12s; DCAE1.1Lite:11299MB/0.06s; DCAE1.1Lite compile: 10385MB/0.03s\nimport torch\nfrom diffusers import AutoencoderDC\n\ntorch._dynamo.config.force_parameter_static_shapes = False\ntorch._dynamo.config.dynamic_shapes = True\ntorch._dynamo.config.recompile_limit = 16\n\nvae = AutoencoderDC.from_pretrained(\"mit-han-lab/dc-ae-lite-f32c32-sana-1.1-diffusers\").to('cuda')\nvae.decode = torch.compile(vae.decode, dynamic=True)\n</code></pre>"},{"location":"sana_sprint/#2-how-to-use-sanasprintpipeline-in-this-repo","title":"2. How to use <code>SanaSprintPipeline</code> in this repo","text":"<pre><code>import torch\nfrom app.sana_sprint_pipeline import SanaSprintPipeline\nfrom torchvision.utils import save_image\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngenerator = torch.Generator(device=device).manual_seed(42)\n\nsana = SanaSprintPipeline(\"configs/sana_sprint_config/1024ms/SanaSprint_1600M_1024px_allqknorm_bf16_scm_ladd.yaml\")\nsana.from_pretrained(\"hf://Efficient-Large-Model/Sana_Sprint_1.6B_1024px/checkpoints/Sana_Sprint_1.6B_1024px.pth\")\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\",\n\nimage = sana(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    guidance_scale=4.5,\n    num_inference_steps=2,\n    generator=generator,\n)\nsave_image(image, 'sana_sprint.png', nrow=1, normalize=True, value_range=(-1, 1))\n</code></pre>"},{"location":"sana_sprint/#how-to-train","title":"How to Train","text":"<pre><code>bash train_scripts/train_scm_ladd.sh \\\n      configs/sana_sprint_config/1024ms/SanaSprint_1600M_1024px_allqknorm_bf16_scm_ladd.yaml\n      --data.data_dir=\"[data/toy_data]\" \\\n      --data.type=SanaWebDatasetMS \\\n      --model.multi_scale=true \\\n      --data.load_vae_feat=true \\\n      --train.train_batch_size=2\n</code></pre>"},{"location":"sana_sprint/#convert-pth-to-diffusers-safetensor","title":"Convert pth to diffusers safetensor","text":"<pre><code>python scripts/convert_scripts/convert_sana_to_diffusers.py \\\n      --orig_ckpt_path Efficient-Large-Model/Sana_Sprint_1.6B_1024px/checkpoints/Sana_Sprint_1.6B_1024px.pth \\\n      --model_type SanaSprint_1600M_P1_D20 \\\n      --scheduler_type scm \\\n      --dtype bf16 \\\n      --dump_path output/Sana_Sprint_1.6B_1024px_diffusers \\\n      --save_full_pipeline\n</code></pre>"},{"location":"sana_sprint/#performance","title":"Performance","text":"Methods (1024x1024) Inference Steps Throughput (samples/s) Latency (s) Params (B) FID \ud83d\udc47 CLIP \ud83d\udc46 GenEval \ud83d\udc46 Sana-Sprint_0.6B 2 6.46 0.25 0.6 6.54 28.40 0.76 Sana-Sprint-1.6B 2 5.68 0.24 1.6 6.50 28.45 0.77"},{"location":"sana_sprint/#citation","title":"Citation","text":"<pre><code>@misc{chen2025sanasprint,\n      title={SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation},\n      author={Junsong Chen and Shuchen Xue and Yuyang Zhao and Jincheng Yu and Sayak Paul and Junyu Chen and Han Cai and Enze Xie and Song Han},\n      year={2025},\n      eprint={2503.09641},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2503.09641},\n}\n</code></pre>"},{"location":"sana_video/","title":"SANA-Video","text":""},{"location":"sana_video/#sana-video-efficient-video-generation-with-block-linear-diffusion-transformer","title":"\ud83c\udfac SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer","text":""},{"location":"sana_video/#demos-of-sana-video","title":"\ud83c\udfac Demos of SANA-Video","text":""},{"location":"sana_video/#about-sana-video","title":"\ud83d\udcfd\ufe0f About SANA-Video","text":"<p>SANA-Video is a small diffusion model designed for efficient video generation, capable of synthesizing high-resolution videos (up to $720 \\times 1280$) and minute-length duration with strong text-video alignment, while maintaining a remarkably fast speed.It enables low-cost, high-quality video generation and can be deployed efficiently on consumer GPUs like the RTX 5090.</p> <p>SANA-Video's Core Contributions:</p> <ul> <li>Efficient Architecture (Linear DiT): Leverages linear attention as the core operation, which is significantly more efficient than vanilla attention for video generation due to the large number of tokens processed.</li> <li>Long-Sequence Capability (Constant-Memory KV Cache): Introduces a Constant-Memory KV cache for Block Linear Attention. This block-wise autoregressive approach uses a fixed-memory state derived from the cumulative properties of linear attention, which eliminates the need for a traditional KV cache, enabling efficient minute-long video generation.</li> <li>Low Training Cost: Achieved effective data filters and model training strategies, narrowing the training cost to only 12 days on 64 H100 GPUs, which is just 1% of the cost of MovieGen.</li> <li>State-of-the-Art Speed and Performance: Achieves competitive performance compared to modern SOTA small diffusion models (e.g., Wan 2.1-1.3B) while being $16\\times$ faster in measured latency\u3002Deployment Acceleration: Can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s ($2.4\\times$ speedup).</li> </ul> <p>In summary, SANA-Video enables high-quality video synthesis at an unmatched speed and low operational cost.</p>"},{"location":"sana_video/#block-causal-linear-attention-causal-mix-ffn-mechanism","title":"\ud83d\udcbb Block Causal Linear Attention &amp;&amp; Causal Mix-FFN Mechanism","text":""},{"location":"sana_video/#how-to-inference","title":"\ud83c\udfc3 How to Inference","text":""},{"location":"sana_video/#1-how-to-use-sana-video-pipelines-in-diffusers","title":"1. How to use Sana-Video Pipelines in <code>\ud83e\udde8diffusers</code>","text":"<p>Note</p> <p>Upgrade your diffusers to use <code>SanaVideoPipeline</code>: &gt; <code>bash     &gt; pip install git+https://github.com/huggingface/diffusers     &gt;</code></p>"},{"location":"sana_video/#text-to-video-sanavideopipeline","title":"Text-to-Video: SanaVideoPipeline","text":"<pre><code>import torch\nfrom diffusers import SanaVideoPipeline\nfrom diffusers import AutoencoderKLWan\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\"\npipe = SanaVideoPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\npipe.vae.to(torch.float32)\npipe.text_encoder.to(torch.bfloat16)\npipe.to(\"cuda\")\nmotion_score = 30\n\nprompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\nnegative_prompt = \"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.\"\nmotion_prompt = f\" motion score: {motion_score}.\"\nprompt = prompt + motion_prompt\n\nvideo = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=480,\n    width=832,\n    frames=81,\n    guidance_scale=6,\n    num_inference_steps=50,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\n\nexport_to_video(video, \"sana_video.mp4\", fps=16)\n</code></pre>"},{"location":"sana_video/#image-to-video-sanaimagetovideopipeline","title":"Image-to-Video: SanaImageToVideoPipeline","text":"<pre><code>import torch\nfrom diffusers import SanaImageToVideoPipeline, FlowMatchEulerDiscreteScheduler\nfrom diffusers.utils import export_to_video, load_image\n\npipe = SanaImageToVideoPipeline.from_pretrained(\"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\")\n# pipe.scheduler = FlowMatchEulerDiscreteScheduler(shift=pipe.scheduler.config.flow_shift)\npipe.transformer.to(torch.bfloat16)\npipe.text_encoder.to(torch.bfloat16)\npipe.vae.to(torch.float32)\npipe.to(\"cuda\")\n\nmotion_score = 30\nprompt = \"A woman stands against a stunning sunset backdrop, her , wavy brown hair gently blowing in the breeze. She wears a veless, light-colored blouse with a deep V-neckline, which ntuates her graceful posture. The warm hues of the setting sun cast a en glow across her face and hair, creating a serene and ethereal sphere. The background features a blurred landscape with soft, ing hills and scattered clouds, adding depth to the scene. The camera ins steady, capturing the tranquil moment from a medium close-up e.\"\nnegative_prompt = \"A chaotic sequence with misshapen, deformed limbs eavy motion blur, sudden disappearance, jump cuts, jerky movements, d shot changes, frames out of sync, inconsistent character shapes, oral artifacts, jitter, and ghosting effects, creating a disorienting al experience.\"\nmotion_prompt = f\" motion score: {motion_score}.\"\nprompt = prompt + motion_prompt\n\nimage = load_image(\"https://raw.githubusercontent.com/NVlabs/Sana//heads/main/asset/samples/i2v-1.png\")\n\noutput = pipe(\n    image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=480,\n    width=832,\n    frames=81,\n    guidance_scale=6,\n    num_inference_steps=50,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\n\nexport_to_video(output, \"sana-ti2v-output.mp4\", fps=16)\n</code></pre>"},{"location":"sana_video/#2-inference-with-txt-file","title":"2. Inference with TXT file","text":""},{"location":"sana_video/#text-to-video","title":"Text-to-Video","text":"<pre><code>bash inference_video_scripts/inference_sana_video.sh \\\n      --np 1 \\\n      --config configs/sana_video_config/Sana_2000M_480px_AdamW_fsdp.yaml \\\n      --model_path hf://Efficient-Large-Model/SANA-Video_2B_480p/checkpoints/SANA_Video_2B_480p.pth \\\n      --txt_file=asset/samples/video_prompts_samples.txt \\\n      --cfg_scale 6 \\\n      --motion_score 30 \\\n      --flow_shift 8 \\\n      --work_dir output/sana_t2v_video_results\n</code></pre>"},{"location":"sana_video/#image-to-video","title":"Image-to-Video","text":"<pre><code>bash inference_video_scripts/inference_sana_video.sh \\\n      --np 1 \\\n      --config configs/sana_video_config/Sana_2000M_480px_AdamW_fsdp.yaml \\\n      --model_path hf://Efficient-Large-Model/SANA-Video_2B_480p/checkpoints/SANA_Video_2B_480p.pth \\\n      --txt_file=asset/samples/sample_i2v.txt \\\n      --task=ltx \\\n      --cfg_scale 6 \\\n      --motion_score 30 \\\n      --flow_shift 8 \\\n      --work_dir output/sana_ti2v_video_results\n</code></pre>"},{"location":"sana_video/#how-to-train","title":"\ud83d\udcbb How to Train","text":"<pre><code># 5s Video Model Pre-Training\nbash train_video_scripts/train_video_ivjoint.sh \\\n      configs/sana_video_config/Sana_2000M_480px_AdamW_fsdp.yaml \\\n      --data.data_dir=\"[data/toy_data]\" \\\n      --train.train_batch_size=1 \\\n      --work_dir=output/sana_video \\\n      --train.num_workers=10 \\\n      --train.visualize=true\n</code></pre>"},{"location":"sana_video/#convert-pth-to-diffusers-safetensor","title":"Convert pth to diffusers safetensor","text":"<pre><code>python scripts/convert_scripts/convert_sana_video_to_diffusers.py --dump_path output/SANA_Video_2B_480p_diffusers --save_full_pipeline\n</code></pre>"},{"location":"sana_video/#performance","title":"Performance","text":""},{"location":"sana_video/#vbench-results-480p-resolution","title":"VBench Results - 480p Resolution","text":""},{"location":"sana_video/#text-to-video_1","title":"Text-to-Video","text":"Methods Latency (s) Speedup #Params (B) Total \u2191 Quality \u2191 Semantic / I2V \u2191 MAGI-1 435 1.1\u00d7 4.5 79.18 82.04 67.74 Step-Video 246 2.0\u00d7 30 81.83 84.46 71.28 CogVideoX1.5 111 4.4\u00d7 5 82.17 82.78 79.76 SkyReels-V2 132 3.7\u00d7 1.3 82.67 84.70 74.53 Open-Sora-2.0 465 1.0\u00d7 14 84.34 85.4 80.72 Wan2.1-14B 484 1.0\u00d7 14 83.69 85.59 76.11 Wan2.1-1.3B 103 4.7\u00d7 1.3 83.31 85.23 75.65 SANA-Video 60 8.0\u00d7 2 84.17 84.85 81.46"},{"location":"sana_video/#image-to-video_1","title":"Image-to-Video","text":"Methods Latency (s) Speedup #Params (B) Total \u2191 Quality \u2191 Semantic / I2V \u2191 MAGI-1 435 1.1\u00d7 4.5 89.28 82.44 96.12 Step-Video-TI2V 246 2.0\u00d7 30 88.36 81.22 95.50 CogVideoX-5b-I2V 111 4.4\u00d7 5 86.70 78.61 94.79 HunyuanVideo-I2V 210 2.3\u00d7 13 86.82 78.54 95.10 Wan2.1-14B 493 1.0\u00d7 14 86.86 80.82 92.90 SANA-Video 60 8.2\u00d7 2 88.02 79.65 96.40"},{"location":"sana_video/#vbench-results-720p-resolution","title":"VBench Results - 720p Resolution","text":"Models Latency (s) Total \u2191 Quality \u2191 Semantic \u2191 Wan-2.1-14B 1897 83.73 85.77 75.58 Wan-2.1-1.3B 400 83.38 85.67 74.22 Wan-2.2-5B 116 83.28 85.03 76.28 SANA-Video-2B 36 84.05 84.63 81.73 <p>Summary: Compared with the current SOTA small video models, SANA's performance is very competitive and speed is much faster. SANA provides 83.71 VBench overall performance with only 2B model parameters, 16\u00d7 acceleration at 480p, and achieves 84.05 total score with only 36s latency at 720p resolution.</p>"},{"location":"sana_video/#vbench-results-30s-long-video-vbench","title":"VBench Results - 30s Long Video Vbench","text":"Models FPS Total \u2191 Quality \u2191 Semantic \u2191 SkyReels-V2 0.49 75.29 80.77 53.37 FramePack 0.92 81.95 83.61 75.32 Self-Forcing 17.0 81.59 83.82 72.70 LongSANA-2B 27.5 82.29 83.10 79.04 <p>Summary: Compared with the current SOTA long video generation models, LongSANA (SANA-Video + LongLive)'s speed and performance is very competitive. LongSANA's 27FPS generatin speed on H100 makes real-time generation possible.</p>"},{"location":"sana_video/#citation","title":"Citation","text":"<pre><code>@misc{chen2025sana,\n      title={SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer},\n      author={Chen, Junsong and Zhao, Yuyang and Yu, Jincheng and Chu, Ruihang and Chen, Junyu and Yang, Shuai and Wang, Xianbang and Pan, Yicheng and Zhou, Daquan and Ling, Huan and others},\n      year={2025},\n      eprint={2509.24695},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2509.24695},\n}\n</code></pre>"},{"location":"sana_video_inference/","title":"Video Inference","text":""},{"location":"sana_video_inference/#inference-cli","title":"Inference CLI","text":""},{"location":"sana_video_inference/#inference-sana-video","title":"Inference SANA-Video","text":"<pre><code>python app/sana_video_pipeline.py \\\n        --config configs/sana_video_config/480ms/Sana_1600M_480px_adamW_fsdp.yaml \\\n        --model_path \"hf://Efficient-Large-Model/SanaVideo_willquant/checkpoints/model.pth\" \\\n        --save_path sana_video.mp4 \\\n        --prompt \"In a whimsical forest setting, a small deer with antlers stands amidst oversized mushrooms and scattered carrots. The scene is vibrant with lush green moss and rocks, creating a magical atmosphere. The deer appears curious, moving slowly across the ground, surrounded by the towering fungi and colorful vegetables. The sky above is clear and bright, adding to the enchanting ambiance. A low-angle shot captures the deer's gentle exploration of this fantastical landscape.\"\n</code></pre>"},{"location":"sana_video_inference/#inference-sana-video-chunked-version","title":"Inference SANA-Video Chunked Version","text":"<pre><code>python app/sana_video_pipeline.py \\\n        --config configs/sana_video_config/480ms/Sana_1600M_480px_adamW_fsdp_chunk.yaml \\\n        --model_path \"hf://Efficient-Large-Model/SanaVideo_chunk/checkpoints/model.pth\" \\\n        --save_path sana_video_chunk_i2v.mp4 \\\n        --interval_k 0.2 \\\n        --image_path output/tmp_videos/wan_goodcase_i2v_eval/00000000_video_001.jpg \\\n        --prompt \"In a whimsical forest setting, a small deer with antlers stands amidst oversized mushrooms and scattered carrots. The scene is vibrant with lush green moss and rocks, creating a magical atmosphere. The deer appears curious, moving slowly across the ground, surrounded by the towering fungi and colorful vegetables. The sky above is clear and bright, adding to the enchanting ambiance. A low-angle shot captures the deer's gentle exploration of this fantastical landscape.\"\n</code></pre>"},{"location":"ComfyUI/comfyui/","title":"ComfyUI","text":""},{"location":"ComfyUI/comfyui/#sana-comfyui","title":"\ud83d\udd8c\ufe0f Sana-ComfyUI","text":"<p>Original Repo</p>"},{"location":"ComfyUI/comfyui/#model-info-implementation","title":"Model info / implementation","text":"<ul> <li>Uses Gemma2 2B as the text encoder</li> <li>Multiple resolutions and models available</li> <li>Compressed latent space (32 channels, /32 compression) - needs custom VAE</li> </ul>"},{"location":"ComfyUI/comfyui/#usage","title":"Usage","text":"<ol> <li>All the checkpoints will be downloaded automatically.</li> <li>KSampler(Flow Euler) is available for now; Flow DPM-Solver will be available soon.</li> </ol> <pre><code>git clone https://github.com/comfyanonymous/ComfyUI.git\ncd ComfyUI\ngit clone https://github.com/lawrence-cj/ComfyUI_ExtraModels.git custom_nodes/ComfyUI_ExtraModels\n\npython main.py\n</code></pre>"},{"location":"ComfyUI/comfyui/#a-sample-workflow-for-sana","title":"A sample workflow for Sana","text":"<p>Sana workflow</p> <p></p>"},{"location":"ComfyUI/comfyui/#a-sample-for-t2isana-i2vcogvideox","title":"A sample for T2I(Sana) + I2V(CogVideoX)","text":"<p>Sana + CogVideoX workflow</p> <p></p>"},{"location":"ComfyUI/comfyui/#a-sample-workflow-for-sana-4096x4096-image-18gb-gpu-is-needed","title":"A sample workflow for Sana 4096x4096 image (18GB GPU is needed)","text":"<p>Sana workflow</p> <p></p>"},{"location":"inference_scaling/inference_scaling/","title":"Inference Scaling","text":""},{"location":"inference_scaling/inference_scaling/#inference-time-scaling-for-sana-15","title":"Inference Time Scaling for SANA-1.5","text":"<p>We trained a specialized NVILA-2B model to score images, which we named VISA (VIla as SAna verifier). By selecting the top 4 images from 2,048 candidates, we enhanced the GenEval performance of SD1.5 and SANA-1.5-4.8B v2, increasing their scores from 42 to 87 and 81 to 96, respectively.</p> <p></p> <p>Even for smaller number of candidates, like 32, we can also push the performance over 90% for SANA-1.5-4.8B v2 in the GenEval.</p>"},{"location":"inference_scaling/inference_scaling/#environment-requirement","title":"Environment Requirement","text":"<p>Dependency setups:</p> <pre><code># other transformers version may also work, but we have not tested\npip install transformers==4.46\npip install git+https://github.com/bfshi/scaling_on_scales.git\n</code></pre>"},{"location":"inference_scaling/inference_scaling/#1-generate-n-images-with-a-pth-file-for-the-following-selection","title":"1. Generate N images with a .pth file for the following selection","text":"<pre><code># download the checkpoint for the following generation\nhuggingface-cli download Efficient-Large-Model/Sana_600M_512px --repo-type model --local-dir output/Sana_600M_512px --local-dir-use-symlinks False\n# 32 is a relatively small number for test but can already push the geneval&gt;90% when we verify the SANA-1.5-4.8B v2 model. Set it to larger number like 2048 for the limit of sky.\nn_samples=32\npick_number=4\n\noutput_dir=output/geneval_generated_path\n# example\nbash scripts/infer_run_inference_geneval.sh \\\n    configs/sana_config/512ms/Sana_600M_img512.yaml \\\n    output/Sana_600M_512px/checkpoints/Sana_600M_512px_MultiLing.pth \\\n    --img_nums_per_sample=$n_samples \\\n    --output_dir=$output_dir\n</code></pre>"},{"location":"inference_scaling/inference_scaling/#2-use-nvila-verifier-to-select-from-the-generated-images","title":"2. Use NVILA-Verifier to select from the generated images","text":"<pre><code>bash tools/inference_scaling/nvila_sana_pick.sh \\\n    $output_dir \\\n    $n_samples \\\n    $pick_number\n</code></pre>"},{"location":"inference_scaling/inference_scaling/#3-calculate-the-geneval-metric","title":"3. Calculate the GenEval metric","text":"<p>You need to use the GenEval environment for the final evaluation. The document about installation can be found here.</p> <pre><code># activate geneval env\nconda activate geneval\n\nDIR_AFTER_PICK=\"output/nvila_pick/best_${pick_number}_of_${n_samples}/${output_dir}\"\n\nbash tools/metrics/compute_geneval.sh $(dirname \"$DIR_AFTER_PICK\") $(basename \"$DIR_AFTER_PICK\")\n</code></pre>"}]}