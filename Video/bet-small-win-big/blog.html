<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SANA-Video Technical Report</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        :root {
            --font-sans: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            --font-mono: 'SF Mono', 'Menlo', 'Monaco', 'Courier New', monospace;
            --text-color: #1a1a1a;
            --bg-color: #ffffff;
            --accent-color: #000000;
            --border-color: #e5e5e5;
            --caption-color: #666666;
            --link-color: #0000EE;
        }

        body {
            font-family: var(--font-sans);
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
            -webkit-font-smoothing: antialiased;
        }

        .container {
            max-width: 840px;
            margin: 0 auto;
            padding: 60px 20px;
        }

        a {
            color: var(--text-color);
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 2px;
        }
        
        a:hover {
            opacity: 0.7;
        }

        /* Header Styles */
        header {
            margin-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 20px;
        }

        h1 {
            font-size: 2.2rem;
            font-weight: 700;
            letter-spacing: -0.02em;
            margin-bottom: 0.5rem;
            line-height: 1.2;
        }

        .metadata {
            font-size: 0.95rem;
            color: var(--caption-color);
            margin-top: 1rem;
            display: flex;
            gap: 20px;
            align-items: center;
        }

        /* Content Styles */
        h2 {
            font-size: 1.4rem;
            font-weight: 600;
            margin-top: 3.5rem;
            margin-bottom: 1.2rem;
            letter-spacing: -0.01em;
            border-bottom: 1px solid #f0f0f0;
            padding-bottom: 10px;
        }

        h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin-top: 2.5rem;
            margin-bottom: 0.8rem;
        }

        p {
            margin-bottom: 1.5rem;
            font-size: 1.05rem;
            color: #333;
        }

        strong {
            font-weight: 600;
            color: #000;
        }

        /* Abstract Box */
        .abstract {
            background-color: #f8f9fa;
            padding: 24px;
            border-radius: 6px;
            margin-bottom: 40px;
            border-left: 3px solid #000;
            font-size: 1.0rem;
        }

        /* Images */
        figure {
            margin: 40px 0;
            text-align: center;
        }

        img {
            max-width: 85%;
            height: auto;
            border-radius: 4px;
            display: block;
            margin: 0 auto;
            box-shadow: none;
        }

        figcaption {
            font-size: 0.9rem;
            color: var(--caption-color);
            text-align: center;
            margin-top: 12px;
            line-height: 1.4;
        }

        /* Tables */
        .table-wrapper {
            overflow-x: auto;
            margin: 30px 0;
            border: 1px solid var(--border-color);
            border-radius: 6px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
            font-variant-numeric: tabular-nums;
            background: #fff;
        }

        th {
            text-align: left;
            background-color: #f8f9fa;
            border-bottom: 1px solid var(--border-color);
            padding: 12px 16px;
            font-weight: 600;
            white-space: nowrap;
        }

        td {
            padding: 12px 16px;
            border-bottom: 1px solid var(--border-color);
            color: #444;
        }

        tr:last-child td {
            border-bottom: none;
        }

        /* Buttons */
        .button-group {
            display: flex;
            gap: 15px;
            margin-top: 40px;
            justify-content: center;
        }

        .btn {
            display: inline-block;
            padding: 12px 24px;
            background-color: #000;
            color: #fff;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            font-size: 0.95rem;
            transition: opacity 0.2s;
        }

        .btn:hover {
            opacity: 0.8;
            color: #fff;
        }

        .btn-outline {
            background-color: transparent;
            border: 1px solid #e5e5e5;
            color: #000;
        }
        
        .btn-outline:hover {
            background-color: #f8f8f8;
            color: #000;
        }

        /* Callout */
        .callout {
            border-left: 3px solid #666;
            padding: 15px 20px;
            background-color: #fcfcfc;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }
        
        /* Layout Grid for Images */
        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>

<div class="container">
    <header>
        <h1>Bet Small to Win Big: Efficient 2K Video Generation via Deeper-compression AutoEncoder, Linear Attention and Two-Stage Refiner</h1>
        <div class="metadata">
            <strong>SANA-Video Team</strong>
            <span>&bull;</span>
            <span>February 6, 2026</span>
        </div>
        
        <figure style="margin-top: 40px;">
            <video src="videos/1-stages-compare.mp4" autoplay muted loop playsinline style="width: 95%;"></video>
            <figcaption><strong>Video 1:</strong> Side-by-side comparison of Stage 1 base generation (Left) vs. Stage 2 Refined output (Right). Prompt: <em>&ldquo;An astronaut hatches from a fragile egg on the surface of the Moon, the shell cracking and peeling apart in gentle low-gravity motion &hellip; Ultra-realistic detail, cinematic lighting, and a breath-taking, movie-like shot.&rdquo;</em></figcaption>
        </figure>
    </header>

    <div class="abstract">
        <strong>Abstract:</strong> In the pursuit of optimizing the trade-off between generation latency and visual fidelity, SANA-Video adopts an aggressive spatial compression strategy (\(32 \times 32\)). While this enables the generation of 720p video in under 30 seconds, the high compression ratio introduces inherent artifacts and blurring in motion dynamics. This blog details our transition to a <strong>Two-Stage Inference Paradigm</strong>. By leveraging a high-compression base model for structural discovery (<strong>Stage 1</strong>) and a step-distilled refiner for high-frequency detail injection (<strong>Stage 2</strong>), we achieve 2K resolution output without compromising the latency profile of the original 720p generation.
        <br><br>
        Want to have a quick 2k video generation experience? Here are our <a href="https://github.com/NVlabs/Sana"><strong>Code</strong></a> and <a href="https://sana-video.hanlab.ai/"><strong>720p Demo</strong></a>.
    </div>

    <div class="grid-2" style="margin: 40px 0; grid-template-columns: 3fr 2fr; align-items: end;">
        <figure style="margin: 0;">
            <img src="images/1-detail-comparison.png" alt="Detail Zoom Comparison" style="max-width: 100%;">
            <figcaption><strong>Figure 1:</strong> Visual comparison between Stage 1 (Left) and Stage 2 Refined output (Right).</figcaption>
        </figure>
        <figure style="margin: 0; display: flex; flex-direction: column; justify-content: flex-end;">
            <img src="images/2-benchmark-chart.png" alt="Benchmark Chart" style="max-width: 100%;">
            <figcaption><strong>Figure 2:</strong> Latency comparison on H100 across different pipelines.</figcaption>
        </figure>
    </div>

    <p>
        Before detailing the methodology, we present a direct comparison of the generation stages. <strong>Figure 1</strong> and <strong>Video 1</strong> illustrate the qualitative leap provided by the Refiner. Stage 1 (Left) successfully captures the global motion dynamics but suffers from compression blur. Stage 2 (Right) effectively &ldquo;translates&rdquo; this blurry latent into high-fidelity footage. Note the restoration of critical high-frequency details&mdash;specifically the distinct facial features inside the helmet and the granular texture of the cracking surface&mdash;which are smoothed out in the base generation. <strong>Figure 2</strong> previews our end-to-end latency results, showing that the 2K refined pipeline matches the original 720p generation time.
    </p>

    <h2>1. Motivation: The Compression-Fidelity Trade-off</h2>
    <p>
        The core efficiency of SANA-Video stems from its compact <strong>2B parameter</strong> architecture and <strong>rapid adaptation</strong> capabilities. This is facilitated by utilizing high-compression Variational Autoencoders (VAEs), specifically DCAE-V (\(32 \times 32 \times 4\)) and LTX-VAE (\(32 \times 32 \times 8\)).
    </p>
    <p>
        These architectures allow for near-frictionless latent processing. However, the compression necessitates a loss of high-frequency spatial information, resulting in a characteristic &ldquo;blurry&rdquo; aesthetic in the initial output. Rather than increasing the parameter count of the base model&mdash;which would negate our lightweight advantage&mdash;we introduce a dedicated <strong>Refinement Stage</strong>. This stage functions not merely as a denoiser, but as a latent translator, converting the low-frequency &ldquo;skeleton&rdquo; generated by the base model into a high-fidelity output using a step-distilled sampler.
    </p>
    <p>
        <strong>Betting Small to Win Big</strong>: This approach represents a paradigm shift from &ldquo;Monolithic Scaling&rdquo; to &ldquo;Modular Efficiency.&rdquo; Instead of training a massive single-stage model to handle both structure and texture&mdash;which is computationally expensive&mdash;we decouple the problem. A lightweight Small Model (2B) handles the complex temporal dynamics (Structure), while a specialized Step-Distilled Refiner handles the spatial resolution (Texture). This &ldquo;Small Model + 2-Stage&rdquo; strategy outperforms traditional large models by delivering 2K quality at 720p latency, effectively allowing us to &ldquo;bet small&rdquo; on parameters to &ldquo;win big&rdquo; on performance.
    </p>

    <h2>2. Performance Analysis</h2>
    <p>To validate this paradigm, we conducted extensive benchmarking on NVIDIA H100 hardware.</p>

    <h3>2.1 VAE Architecture Comparison</h3>
    <p>We evaluated three mainstream VAEs at a resolution of \(704 \times 1280 \times 81\). The focus was on the balance between reconstruction quality (Panda70m metrics) and throughput.</p>

    <div class="table-wrapper">
        <table>
            <thead>
                <tr>
                    <th>VAE</th>
                    <th>Latent Size</th>
                    <th>Compression Ratio</th>
                    <th>Enc / Dec Latency (s)</th>
                    <th>GPU Mem</th>
                    <th>Panda70m (PSNR↑/SSIM↑/LPIPS↓)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Wan2.1 (8x8x4, c16)</td>
                    <td>88x160x21</td>
                    <td>48</td>
                    <td>2.9s / 5.0s</td>
                    <td><strong>18GB</strong></td>
                    <td>34.15 / 0.952 / <strong>0.017</strong></td>
                </tr>
                <tr>
                    <td>DCAE-V (32x32x4, c64)</td>
                    <td>22x40x20</td>
                    <td>192</td>
                    <td>5.2s / 5.6s</td>
                    <td>47GB</td>
                    <td><strong>35.03 / 0.953</strong> / 0.019</td>
                </tr>
                <tr>
                    <td><strong>LTX2 (32x32x8, c128)</strong></td>
                    <td>22x40x11</td>
                    <td><strong>192</strong></td>
                    <td><strong>1.3s / 0.9s</strong></td>
                    <td>47GB</td>
                    <td>32.41 / 0.928 / 0.039</td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="callout">
        <strong>Key Insight:</strong> While Wan2.1 offers superior reconstruction metrics, its latent size is 32x larger than LTX2, creating a bottleneck for diffusion inference. LTX2 emerges as the efficiency optimal. Its 0.9s decoding time provides a significant "computational budget," allowing the Refiner to operate within the same total timeframe while utilizing 2-5 inference steps to recover the visual quality lost during compression.
    </div>

    <h3>2.2 Latency Analysis: The "Free" Upgrade</h3>
    <figure>
        <img src="images/3-latency-chart.png" alt="Latency Chart" style="width: 70%; max-width: 70%;">
        <figcaption><strong>Figure 3:</strong> End-to-end latency comparison across different pipelines on H100.</figcaption>
    </figure>
    
    <p>As shown in <strong>Figure 3</strong>, comparing the total end-to-end latency reveals a compelling result. The efficiency gains from the new LTX-based pipeline allow us to generate 2K video in the same time required for the previous 720p pipeline.</p>

    <ul>
        <li><strong>SANA-Video-1.0 (720p):</strong> ~36s</li>
        <li><strong>SANA-Video-LTX (720p):</strong> ~18s (50% Efficiency Gain)</li>
        <li><strong>SANA-Video-LTX-Refiner (2K):</strong> ~36s (Free Lunch!)</li>
    </ul>

    <p>Our upgrade from 720p to 2K is the ultimate free lunch: a massive resolution leap at no cost to user latency. <strong>Figure 4</strong> illustrates the complete two-stage inference pipeline.</p>

    <figure>
        <img src="images/4-pipeline.png" alt="Full Pipeline" style="width: 100%; max-width: 100%;">
        <figcaption><strong>Figure 4:</strong> Full two-stage inference pipeline overview.</figcaption>
    </figure>

    <h2>3. Engineering Implementation</h2>

    <h3>3.1 Post-training Adaptation Strategy</h3>
    <p>
        A significant engineering challenge arose from the architectural mismatch between SANA-Video and the LTX2 Refiner. SANA-Video uses a different VAE with different channel dimensions than the LTX system.
    </p>

    <figure>
        <img src="images/5-vae-adapt.png" alt="VAE Adaptation Strategy"， style="width: 80%; max-width: 80%;">
        <figcaption><strong>Figure 5:</strong> VAE adaptation strategy. Both DCAE-V and LTX-VAE encoders are frozen; only the SANA-Video DiT is fine-tuned with randomly initialized Input Embedding and Output layers.</figcaption>
    </figure>

    <p>
        To resolve this, we employed a rapid fine-tuning adaptation strategy on the SANA-Video 2B model. As illustrated in <strong>Figure 5</strong>, during the VAE adaptation phase, both the original DCAE-V encoder and the target LTX-VAE encoder remain frozen. Only the SANA-Video DiT model is fine-tuned, with its Input Embedding layer and Output layer randomly re-initialized to match the LTX-VAE latent channel dimension (\(c=128\)). This minimal re-initialization preserves the learned motion priors while enabling compatibility with the new latent space:
    </p>
    <ol>
        <li><strong>Initialization:</strong> We randomly initialized only the <strong><em>Patch Embedding</em></strong> layer and the <strong><em>Final Output</em></strong> layer to accommodate the LTX-VAE latent channels (\(c=128\)).</li>
        <li><strong>Convergence:</strong> Training logs indicate that semantic information is recovered within <strong>500 steps</strong>, with full color and texture restoration achieved by <strong>5,000 steps</strong>, as shown in <strong>Figure 6</strong>.</li>
    </ol>

    <p>
        This decoupling allows SANA-Video to handle the heavy lifting of motion logic (Structure), while the LTX2 Refiner handles the &ldquo;rendering&rdquo; (Texture), creating a universal, plug-and-play enhancement pipeline.
    </p>

    <figure>
        <img src="images/6-refiner-loss-vis.png" alt="Refiner Results">
        <figcaption><strong>Figure 6:</strong> Training loss curve and visual quality progression during LTX-VAE adaptation.</figcaption>
    </figure>

    <h3>3.2 The Step-Distilled Refiner</h3>
    <p>
        The LTX2 Refiner in Stage 2 utilizes <strong>Step Distillation</strong>, enabling it to operate without a full denoising trajectory. Instead, it inherits the knowledge distribution of the teacher model to inject high-frequency textures into the Stage 1 output in just <strong>4 steps</strong>. This design ensures the marginal cost of the second stage remains negligible.
    </p>
    <p>
        Crucially, this model is <strong>not a standalone generator</strong>. As illustrated in <strong>Video 2</strong>, our experiments initializing generation from pure Gaussian noise using the 4-step schedule resulted in severe structural collapse and semantic incoherence. Consequently, the Refiner&rsquo;s function is strictly defined as <strong>Latent Adaptation and Enhancement</strong>&mdash;it requires the structural prior provided by the Stage 1 output to perform a high-fidelity translation, rather than generating content from scratch.
    </p>

    <figure>
        <video src="videos/2-refiner-failure.mp4" autoplay muted loop playsinline style="width: 50%;"></video>
        <figcaption><strong>Video 2:</strong> Attempting to generate video from pure Gaussian noise only using the Refiner (4 steps) results in structural collapse and semantic failure. Prompt: <em>A cat and a dog baking a cake together in a kitchen.</em></figcaption>
    </figure>

    <h3>3.3 Disable the Audio Branch via Zero-Initialization</h3>
    <p>
        Since the LTX2 architecture employs a dual-branch DiT for joint video and audio generation, the Stage 2 refiner inherently anticipates concurrent inputs for both Video and Audio Latents. However, current open-source video models, including SANA-Video, typically lack audio generation capabilities. The adaptation of the LTX refiner for such <strong>video-only architectures</strong> remains an under-explored challenge.
    </p>
    <p>
        We propose a practical solution to bypass this dependency. We strictly initialize the audio input as a <strong>zero-tensor</strong> within the refiner. Empirically, we found that injecting noise at the default scale within the scheduler effectively simulates the necessary latent distribution. This strategy allows us to leverage the refiner's visual enhancement capabilities without requiring an additional audio signal or a pre-existing audio branch.
    </p>

    <h2>4. Conclusion & Future Work</h2>
    <p>
        This update demonstrates the potential of video generation with workflow decoupling rather than with monolithic scaling. The Two-Stage Inference paradigm&mdash;&ldquo;Fast Structure&rdquo; followed by &ldquo;Beautiful Detail&rdquo;&mdash;solves the high-compression artifact issue while maintaining SANA-Video's signature speed.
    </p>
    <p>
        <strong>Future Roadmap:</strong> We are optimizing the training scheme for the refiner and plan to open-source the complete lightweight refiner training workflow to the community.
    </p>
    
    <h2>Citation</h2>
    <p>If you find our work helpful, please consider citing:</p>
    <pre style="background-color: #f8f9fa; padding: 16px; border-radius: 6px; font-size: 0.85rem; overflow-x: auto; line-height: 1.5; color: #333;">@article{sana_video_with_refiner,
    title={Bet Small to Win Big: Efficient 2K Video Generation via Deeper-compression AutoEncoder, Linear Attention and Two-Stage Refiner},
    author={SANA-Video Team},
    year={2026},
    month={Feb},
    url={https://nvlabs.github.io/Sana/Video/blog/blog.html}
    }</pre>

    <h3>References</h3>
    <p style="font-size: 0.9rem; color: #666;">
        1. LTX-2: Efficient Joint Audio-Visual Foundation Model<br>
        2. Waver: Wave Your Way to Lifelike Video Generation<br>
        3. FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space<br>
        4. SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer
    </p>

    <div class="button-group">
        <a href="https://github.com/NVlabs/Sana" class="btn">View on GitHub</a>
        <a href="https://sana-video.hanlab.ai/" class="btn btn-outline">Try the 2K Demo</a>
    </div>

    <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #eee; font-size: 0.8rem; color: #888; text-align: center;">
        &copy; 2026 SANA-Video Team. All rights reserved.
    </footer>
</div>

<script>
    function sendHeight() {
        const height = document.body.scrollHeight;
        window.parent.postMessage({ height: height }, '*');
    }
    
    window.addEventListener('load', sendHeight);
    window.addEventListener('resize', sendHeight);
    
    window.addEventListener('load', function() {
        if (window.MathJax) {
            MathJax.startup.promise.then(function() {
                sendHeight();
            });
        }
        
        setTimeout(sendHeight, 500);
    });
</script>

</body>
</html>